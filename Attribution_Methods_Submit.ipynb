{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y5YZsOZ4E6g6"
   },
   "source": [
    " # Git "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/DmitryUlyanov/deep-image-prior\n",
    "!mkdir deep_image_prior\n",
    "!mv ./deep-image-prior/* ./deep_image_prior/\n",
    "!rm -rf ./deep-image-prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l2jiL4BmGcdQ"
   },
   "source": [
    "# Models & Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NyxE5FKAJDRo"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.autograd import Function\n",
    "from torchvision import models\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pickle \n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils import *\n",
    "from deep_image_prior.utils.perceptual_loss.perceptual_loss import *\n",
    "from deep_image_prior.utils.common_utils import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "ed66bd634cdf419b809ba2b36d18b27d",
      "c6d4bd3ab87249b39e848b2376defa5e",
      "a13fb52ab664465392a464701fbe867e",
      "61f528ba67434ea8bf28d992315e131b",
      "a0ad0c5bb48d4e6c8108ea7365d169da",
      "a813be12cbd14e439ae666417d5ed940",
      "e2946d57503a4eb680e50b4601d05f7c",
      "d65653601f63465db95c275bb66eb80c",
      "1468f45d121d4ee1998aabd36ca64ceb",
      "aceaeee5bab84f548c7a628726643c10",
      "0dd547c66aed405a8284d21742c18389"
     ]
    },
    "id": "vvhkuuhvHMW3",
    "outputId": "e8666d34-323c-48c0-9fe4-23f5602ee33b"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "%matplotlib inline\n",
    "\n",
    "import argparse\n",
    "from deep_image_prior.models import *\n",
    "import torch.optim\n",
    "torch.backends.cudnn.enabled = True\n",
    "\n",
    "dtype = torch.cuda.FloatTensor\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Choose net type\n",
    "import torchvision.models as models\n",
    "# pretrained_net = 'vgg19_caffe' \n",
    "# assert pretrained_net in ['alexnet_caffe', 'vgg19_caffe', 'vgg16_caffe']\n",
    "pretrained_net = models.resnet18(pretrained=True)\n",
    "pretrained_net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fRqrSHZNHZWe"
   },
   "outputs": [],
   "source": [
    "def get_resnet_preprocessor(imsize):\n",
    "    preprocess = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "    return preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cHb2zBOtGYJY"
   },
   "outputs": [],
   "source": [
    "# Target imsize \n",
    "imsize = 227 if pretrained_net == 'alexnet_caffe' else 224\n",
    "\n",
    "# Something divisible by a power of two\n",
    "imsize_net = 256\n",
    "\n",
    "# VGG and Alexnet need input to be correctly normalized\n",
    "preprocess = get_resnet_preprocessor(imsize)\n",
    "\n",
    "cnn = pretrained_net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EZJrF2RyJlw2"
   },
   "source": [
    "# Setting Work Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KtH3dfsSdYwG",
    "outputId": "000f60f4-a300-4486-815c-fa38b9cc4efa"
   },
   "outputs": [],
   "source": [
    "dir = '/media/heatmaps/null'\n",
    "\n",
    "# Change before run.\n",
    "data_dir = '/media/data/null/Data'\n",
    "\n",
    "# Choose the setting for generating image samples.\n",
    "image_constructors = {\n",
    "    'repeated_patch_4': repeated_patch_image,\n",
    "    'repeated_patch_2': repeated_two_patch,\n",
    "    'single_patch': single_patch_image_constructor,\n",
    "    'two_patch': image_constructor}\n",
    "\n",
    "mode = 'two_patch'\n",
    "assert(mode in image_constructors)\n",
    "\n",
    "patches_files = [f for f in listdir(data_dir) if isfile(join(data_dir, f))]\n",
    "data_list = []\n",
    "\n",
    "count_ = 0\n",
    "for file_name in patches_files:    \n",
    "  file_dir = join(data_dir, file_name)\n",
    "  f = open(file_dir, \"rb\")\n",
    "  # [patch_positions, random_bg, map_idxs]  \n",
    "  data_list += [pickle.load(f)]\n",
    "  f.close()\n",
    "\n",
    "images = []\n",
    "for data in data_list:\n",
    "  images += image_constructors[mode](data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KrNoFbv0w_pW"
   },
   "outputs": [],
   "source": [
    "def save_data_image(dir, name, hm, step):\n",
    "\n",
    "  heatmap = hm.squeeze().cpu()\n",
    "  if len(heatmap.shape) != 2:\n",
    "    if heatmap.shape[0] == 3:\n",
    "        heatmap = torch.mean(heatmap, 0).squeeze()\n",
    "    else:\n",
    "        heatmap = torch.mean(heatmap, 2).squegeze()\n",
    "  \n",
    "  if torch.max(abs(heatmap)) > 1:\n",
    "      heatmap /= 255\n",
    "\n",
    "  data_dir = join(dir, 'data')\n",
    "  os.makedirs(data_dir, exist_ok=True)\n",
    "  full_dir = join(data_dir, name)\n",
    "  f = open(full_dir, 'wb')\n",
    "  pickle.dump(heatmap, f)\n",
    "  f.close()\n",
    "    \n",
    "  heatmap = abs(heatmap)\n",
    "  heatmap = torch.clamp(heatmap*1.5, min=0, max=1)\n",
    "    \n",
    "  image_number = int(name.split('_')[-1].split('.')[1])\n",
    "  if image_number < 50: \n",
    "    image_dir = join(dir, 'images')\n",
    "    os.makedirs(image_dir, exist_ok=True)\n",
    "    plt.imsave(join(image_dir, name + '.png'), heatmap.numpy(), cmap='Greys', format='png')\n",
    "    plt.imsave(join(image_dir, name + '.pdf'), heatmap.numpy(), cmap='Greys', format='pdf')\n",
    "\n",
    "def visualize_image(img):\n",
    "  plt.imshow(img.cpu().permute(1, 2, 0).detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OLB9IN2yI98Q"
   },
   "source": [
    "# GradCAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GlQhD7vYI_Zs"
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/jacobgil/pytorch-grad-cam.git\n",
    "!pip install ttach\n",
    "!pip install grad-cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s_d1OjuLJEzW"
   },
   "outputs": [],
   "source": [
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FcExyITgKS09"
   },
   "outputs": [],
   "source": [
    "def attrib_GradCAM(target_idx):\n",
    "    target_dir_name = 'target'\n",
    "    if target_idx == 1:\n",
    "        target_dir_name = 'second_target'\n",
    "        \n",
    "    target_dir = os.path.join(dir, 'GradCAM/', target_dir_name)\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "    \n",
    "    imsize = 227 if pretrained_net == 'alexnet_caffe' else 224\n",
    "    cnn = models.resnet18(pretrained=True).to(device).eval()\n",
    "    counter = 0\n",
    "\n",
    "    for input_tensor in tqdm(images):\n",
    "\n",
    "      input_tensor.grad = None    \n",
    "      cnn.zero_grad()\n",
    "      grad_cam = GradCAM(model=cnn, target_layers=[cnn.layer4[-1]], use_cuda=True)\n",
    "      input_tensor = torch.tensor(input_tensor).reshape(1, 3, imsize, imsize).cuda()\n",
    "      input = preprocess(input_tensor)\n",
    "      input.requires_grad = True\n",
    "\n",
    "      target_index = int(data_list[counter][2][target_idx])\n",
    "\n",
    "      mask = grad_cam(input, target_index)\n",
    "      mask = mask[0, :]\n",
    "\n",
    "      img_bgr = show_cam_on_image(input_tensor.squeeze().permute(1, 2, 0).cpu().numpy(), mask, use_rgb=True)\n",
    "      hm_name = str(counter) + \"_target:\" + str(target_index) + \"_\" + patches_files[counter]\n",
    "\n",
    "      save_data_image(target_dir, hm_name, torch.Tensor(mask), counter)\n",
    "      counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attrib_GradCAM(0)\n",
    "attrib_GradCAM(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9HWxkwj4KXmV"
   },
   "source": [
    "# GradCAMPlusPlus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Al0dUTENKaWp"
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/jacobgil/pytorch-grad-cam.git\n",
    "!pip install ttach\n",
    "!pip install grad-cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RqagKSc3Kcqu"
   },
   "outputs": [],
   "source": [
    "from pytorch_grad_cam import GradCAMPlusPlus\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_cULYFH2KclU"
   },
   "outputs": [],
   "source": [
    "def attrib_GradCAMPlusPlus(target_idx):\n",
    "    target_dir_name = 'target'\n",
    "    if target_idx == 1:\n",
    "        target_dir_name = 'second_target'\n",
    "    \n",
    "    for l in range(4):\n",
    "        target_dir = os.path.join(dir, 'GradCAMPlusPlus/', 'layer'+str(l+1), target_dir_name)\n",
    "        os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "        imsize = 227 if pretrained_net == 'alexnet_caffe' else 224\n",
    "        cnn = models.resnet18(pretrained=True).to(device).eval()\n",
    "        counter = 0\n",
    "\n",
    "        for input_tensor in tqdm(images):\n",
    "\n",
    "          input_tensor.grad = None    \n",
    "          cnn.zero_grad()\n",
    "          grad_target_layers = [cnn.layer1[-1], cnn.layer2[-1], cnn.layer3[-1], cnn.layer4[-1]]            \n",
    "          grad_cam = GradCAMPlusPlus(model=cnn, target_layers=[grad_target_layers[l]], use_cuda=True)\n",
    "          input_tensor = torch.tensor(input_tensor).reshape(1, 3, imsize, imsize).cuda()\n",
    "          input = preprocess(input_tensor)\n",
    "          input.requires_grad = True\n",
    "\n",
    "          target_index = int(data_list[counter][2][target_idx])\n",
    "\n",
    "          mask = grad_cam(input, target_index)\n",
    "          mask = mask[0, :]\n",
    "\n",
    "          img_bgr = show_cam_on_image(input_tensor.squeeze().permute(1, 2, 0).cpu().numpy(), mask)\n",
    "\n",
    "          hm_name = str(counter) + \"_target:\" + str(target_index) + \"_\" + patches_files[counter]\n",
    "\n",
    "          save_data_image(target_dir, hm_name, torch.Tensor(mask), counter)\n",
    "          counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attrib_GradCAMPlusPlus(0)\n",
    "attrib_GradCAMPlusPlus(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r-KWxv-6KrwG"
   },
   "source": [
    "# FullGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6WAVi9wbKrmV"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/idiap/fullgrad-saliency.git\n",
    "!mv fullgrad-saliency/* ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vAP-LIoBKrkE"
   },
   "outputs": [],
   "source": [
    "from saliency.fullgrad import FullGrad\n",
    "from misc_functions import NormalizeInverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z1zWmu9DKrhV"
   },
   "outputs": [],
   "source": [
    "from misc_functions import *\n",
    "def attrib_FullGrad(target_idx):\n",
    "    \n",
    "    target_dir_name = 'target'\n",
    "    if target_idx == 1:\n",
    "        target_dir_name = 'second_target'\n",
    "        \n",
    "    target_dir = os.path.join(dir, 'FullGrad/', target_dir_name)\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "    imsize = 227 if pretrained_net == 'alexnet_caffe' else 224\n",
    "    \n",
    "    cnn = models.resnet18(pretrained=True).to(device).eval()\n",
    "    counter = 0\n",
    "\n",
    "    unnormalize = NormalizeInverse(mean = [0.485, 0.456, 0.406],\n",
    "                               std = [0.229, 0.224, 0.225])\n",
    "    fullgrad = FullGrad(cnn)\n",
    "    for input_tensor in tqdm(images):\n",
    "\n",
    "      cnn.zero_grad()\n",
    "\n",
    "      input_tensor = torch.tensor(input_tensor).permute(2, 0, 1).reshape(1, 3, imsize, imsize).cuda()\n",
    "      input = preprocess(input_tensor)\n",
    "      input.requires_grad = True\n",
    "\n",
    "      target_index = int(data_list[counter][2][target_idx])\n",
    "      target_index_tensor = torch.Tensor([[target_index]]).type(torch.int64).cuda()\n",
    "\n",
    "\n",
    "      mask = fullgrad.saliency(input, target_index_tensor)\n",
    "      mask = mask[0]\n",
    "\n",
    "      hm_name = str(counter) + \"_target:\" + str(target_index) + \"_\" + patches_files[counter]\n",
    "      save_data_image(target_dir, hm_name, torch.tensor(mask), counter)\n",
    "      counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attrib_FullGrad(0)\n",
    "attrib_FullGrad(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GExOnS88NZoL"
   },
   "source": [
    "# IBA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oHxlEGx9Nbsr"
   },
   "source": [
    "## Download ImageNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0quDUxQENUol",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%mkdir /content/data\n",
    "%cd /content/data\n",
    "!wget -c https://image-net.org/data/ILSVRC/2012/ILSVRC2012_img_val.tar &\n",
    "%mkdir validation && mv ILSVRC2012_img_val.tar validation/ && cd validation && tar -xvf ILSVRC2012_img_val.tar\n",
    "%cd validation\n",
    "!sh /content/drive/MyDrive/CAMP/ImageNet-Scripts/valprep.sh\n",
    "%cd /content/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KB6RKFGKNf9r"
   },
   "source": [
    "## Actual Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tl4OlqqeNUmT"
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/BioroboticsLab/IBA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IBA.pytorch import IBA, tensor_to_np_img, get_imagenet_folder, imagenet_transform\n",
    "from IBA.utils import plot_saliency_map, to_unit_interval, load_monkeys\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models\n",
    "import torch\n",
    "\n",
    "def attrib_IBA(target_idx):\n",
    "    \n",
    "    target_dir_name = 'target'\n",
    "    if target_idx == 1:\n",
    "        target_dir_name = 'second_target'\n",
    "        \n",
    "    for l in range(3, 4):\n",
    "        target_dir = os.path.join(dir, 'IBA/', 'layer'+str(l+1), target_dir_name)\n",
    "        os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "        counter = 0\n",
    "\n",
    "        imagenet_dir = '/media/validation'\n",
    "\n",
    "        # Load model\n",
    "        device = 'cuda:0' if  torch.cuda.is_available() else 'cpu'\n",
    "        model = models.resnet18(pretrained=True).to(device).eval()\n",
    "\n",
    "        imsize = 227 if model == 'alexnet_caffe' else 224\n",
    "        \n",
    "        # Add a Per-Sample Bottleneck at layer conv4_1\n",
    "        grad_target_layers = [model.layer1[-1], model.layer2[-1], model.layer3[-1], model.layer4[-1]] \n",
    "        iba = IBA(grad_target_layers[l])\n",
    "\n",
    "        # Estimate the mean and variance of the feature map at this layer.\n",
    "        val_set = get_imagenet_folder(imagenet_dir)\n",
    "        val_loader = DataLoader(val_set, batch_size=64, shuffle=True, num_workers=4)\n",
    "        iba.estimate(model, val_loader, n_samples=5000, progbar=True)\n",
    "\n",
    "        for input_tensor in tqdm(images):\n",
    "\n",
    "          model.zero_grad()\n",
    "\n",
    "          input_tensor = torch.tensor(input_tensor).reshape(1, 3, imsize, imsize).cuda()\n",
    "          input = preprocess(input_tensor)\n",
    "          input.requires_grad = True\n",
    "\n",
    "          target_index = int(data_list[counter][2][target_idx])\n",
    "          model_loss_closure = lambda x: -torch.log_softmax(model(x), 1)[:, target_index].mean()\n",
    "\n",
    "          saliency_map = iba.analyze(input.to(device), model_loss_closure, beta=10)\n",
    "\n",
    "          model_loss_closure = lambda x: -torch.log_softmax(model(x), 1)[:, target_index].mean()\n",
    "          heatmap = iba.analyze(input.to(device), model_loss_closure )\n",
    "\n",
    "          hm_name = str(counter) + \"_target:\" + str(target_index) + \"_\" + patches_files[counter]\n",
    "          save_data_image(target_dir, hm_name, torch.tensor(heatmap), counter)\n",
    "          counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attrib_IBA(0)\n",
    "attrib_IBA(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qbjozI5gv-yG"
   },
   "source": [
    "# GuidedBackProp, IntegratedGradients, DeepLiftShap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ttpwOAuCKrel"
   },
   "outputs": [],
   "source": [
    "!pip install captum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pNoee3cOKcjF"
   },
   "outputs": [],
   "source": [
    "from captum.attr import GuidedBackprop\n",
    "from captum.attr import DeepLiftShap\n",
    "from captum.attr import IntegratedGradients\n",
    "from tqdm.notebook import tqdm\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hupuhc4exQOR"
   },
   "outputs": [],
   "source": [
    "from resnet import *\n",
    "def attrib_captum(target_idx, method):\n",
    "    \n",
    "    assert method in ['IntegratedGradients', 'GuidedBackProp', 'DeepLiftShap']\n",
    "    \n",
    "    target_dir_name = 'target'\n",
    "    if target_idx == 1:\n",
    "        target_dir_name = 'second_target'\n",
    "    \n",
    "    target_dir = os.path.join(dir, method, target_dir_name)\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "    imsize = 227 if pretrained_net == 'alexnet_caffe' else 224\n",
    "    counter = 0\n",
    "    \n",
    "    \n",
    "    for input_tensor in tqdm(images):\n",
    "\n",
    "      if method == 'DeepLiftShap':\n",
    "          #For DeepLiftShape, place resnet.py and replace cnn with the one below.\n",
    "          cnn = resnet18(pretrained=True).to(device)\n",
    "      else:    \n",
    "          cnn = models.resnet18(pretrained=True).to(device).eval()\n",
    "\n",
    "      cnn.zero_grad()\n",
    "\n",
    "      input_tensor = torch.tensor(input_tensor).reshape(1, 3, imsize, imsize).cuda()\n",
    "      input = preprocess(input_tensor)\n",
    "      input.requires_grad = True\n",
    "\n",
    "      target_index = int(data_list[counter][2][target_idx])\n",
    "\n",
    "      if method == 'IntegratedGradients':        \n",
    "          mask = IntegratedGradients(cnn).attribute(input, target=target_index)\n",
    "      elif method == 'GuidedBackProp':                \n",
    "          mask = GuidedBackprop(cnn).attribute(input, target=target_index)\n",
    "      elif method == 'DeepLiftShap':                \n",
    "          base_dist = torch.clamp(torch.zeros([10, 3, imsize, imsize]).normal_(mean=0.5, std=0.1).type(dtype), 0, 1).to(device)\n",
    "          mask = DeepLiftShap(cnn).attribute(input, target=target_index, baselines=base_dist)\n",
    "\n",
    "      mask = mask[0, :]\n",
    "      mask = mask/torch.max(mask)\n",
    "\n",
    "      hm_name = str(counter) + \"_target:\" + str(target_index) + \"_\" + patches_files[counter]\n",
    "      save_data_image(target_dir, hm_name, mask.detach(), counter)\n",
    "      counter += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = ['IntegratedGradients', 'GuidedBackProp', 'DeepLiftShap']\n",
    "\n",
    "for method in methods:\n",
    "    for target in [0, 1]:\n",
    "        print(method, 'target:', target)\n",
    "        attrib_captum(target, method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4MR1FZRLBHJa"
   },
   "source": [
    "# GuidedBackProp, Gradient, ExtreamlPertubation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rE4HXolQBG_7",
    "outputId": "fcf84cb3-0dc3-466b-e5bb-c1987d1e647c"
   },
   "outputs": [],
   "source": [
    "!pip install torchray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wTaFuOgSBU5H"
   },
   "outputs": [],
   "source": [
    "from torchray.attribution.guided_backprop import guided_backprop\n",
    "from torchray.attribution.extremal_perturbation import extremal_perturbation\n",
    "from torchray.attribution.grad_cam import grad_cam\n",
    "from torchray.attribution.gradient import gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "b61db78ba6f64e57b873498c9e5b7432",
      "c5fbf08d5ddc405cb224d8f1b481c958",
      "92c042e83f6f4090b0d8d667b69a5600",
      "9ec04d0f8bd344e39e100d1c1fd72c23",
      "5be051f7ed1c489fae45d6127e8f8829",
      "0b23b604a6ea48f69b0130008fe73572",
      "02264d3a2b8e4ba1a71b7e2e357225a8",
      "822add6865e8404b9137da5827fc6875",
      "32e3bce25b82425f84089a6d59fd478d",
      "cee91690cbd44a8a855da67ce1c0a107",
      "ebc276c765c34f31bbb25fb8133eaa44"
     ]
    },
    "id": "zwXAYXF-B1Ln",
    "outputId": "05880273-f733-4737-dae3-40ecdf0e58ff"
   },
   "outputs": [],
   "source": [
    "def attrib_torchray(target_idx, method):\n",
    "    \n",
    "    assert method in ['ExtremalPertubation', 'Gradient']\n",
    "    \n",
    "    target_dir_name = 'target'\n",
    "    if target_idx == 1:\n",
    "        target_dir_name = 'second_target'\n",
    "    \n",
    "    target_dir = os.path.join(dir, method, target_dir_name)\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "    imsize = 227 if pretrained_net == 'alexnet_caffe' else 224\n",
    "    cnn = models.resnet18(pretrained=True).to(device).eval()\n",
    "    counter = 0\n",
    "\n",
    "    for input_tensor in tqdm(images):\n",
    "      cnn.zero_grad()\n",
    "\n",
    "      input_tensor = torch.tensor(input_tensor).reshape(1, 3, imsize, imsize).cuda()\n",
    "      input = preprocess(input_tensor)\n",
    "      input.requires_grad = True\n",
    "\n",
    "      target_index = int(data_list[counter][2][target_idx])\n",
    "\n",
    "      if method == 'Gradient':\n",
    "        mask = gradient(cnn, input, target_index)\n",
    "      elif method == 'ExtremalPertubation':\n",
    "        mask, _ = extremal_perturbation(cnn, input, target_index)\n",
    "        \n",
    "      mask = mask[0, :]\n",
    "      mask = mask/torch.max(mask)\n",
    "\n",
    "      hm_name = str(counter) + \"_target:\" + str(target_index) + \"_\" + patches_files[counter]\n",
    "      save_data_image(target_dir, hm_name, mask, counter)\n",
    "      counter += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = ['ExtremalPertubation', 'Gradient']\n",
    "\n",
    "for method in methods:\n",
    "    for target in [0, 1]:\n",
    "        print(method, 'target:', target)\n",
    "        attrib_torchray(target, method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t2SRkLv5VNOv"
   },
   "source": [
    "# Visaulize Heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iro26y_LFt7j"
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pickle \n",
    "\n",
    "data_dir = join(target_dir, 'data')\n",
    "\n",
    "hm_files = [f for f in listdir(data_dir) if isfile(join(data_dir, f))]\n",
    "data = []\n",
    "\n",
    "for file_name in hm_files:\n",
    "  file_dir = join(data_dir, file_name)\n",
    "  f = open(file_dir, \"rb\")\n",
    "  data += [pickle.load(f)]\n",
    "  f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_heatmap(hm):\n",
    "  \n",
    "  heatmap = hm.squeeze().cpu()\n",
    "  if len(heatmap.shape) != 2:\n",
    "    heatmap = torch.sum(heatmap, 0).squeeze()\n",
    "  \n",
    "  plt.imshow(heatmap.numpy(), cmap='Reds') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_heatmap(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "id": "pVcOljbymKh7",
    "outputId": "adc8be59-8649-49ab-a48e-6053871dc22b"
   },
   "source": [
    "# Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Method: Class Sensitivity, Null Player')\n",
    "\n",
    "\n",
    "method_dir = '/media/heatmaps/double'\n",
    "methods = ['DeepLiftShap', 'ExtremalPertubation', 'FullGrad', 'GradCAM', 'Gradient',\n",
    "          'GuidedBackProp', 'IntegratedGradients', 'GradCAMPlusPlus/layer1',\n",
    "          'GradCAMPlusPlus/layer2', 'GradCAMPlusPlus/layer3', 'GradCAMPlusPlus/layer4', 'IBA/layer1',\n",
    "          'IBA/layer2', 'IBA/layer3', 'IBA/layer4']\n",
    "\n",
    "\n",
    "for m in methods:\n",
    "    first_target_dir = join(method_dir, m, 'target/data')\n",
    "    second_target_dir = join(method_dir, m, 'second_target/data')\n",
    "    \n",
    "    #################### READ HEATMAPS ####################\n",
    "    first_hm_files = sorted([f for f in listdir(first_target_dir) if isfile(join(first_target_dir, f))])\n",
    "    first_hm_list = []\n",
    "\n",
    "    for file_name in sorted(first_hm_files):\n",
    "      file_dir = join(first_target_dir, file_name)\n",
    "      f = open(file_dir, \"rb\")\n",
    "      first_hm_list += [pickle.load(f)]\n",
    "      f.close()\n",
    "        \n",
    "    second_hm_files = sorted([f for f in listdir(second_target_dir) if isfile(join(second_target_dir, f))])\n",
    "    second_hm_list = []\n",
    "\n",
    "    for file_name in sorted(second_hm_files):\n",
    "      file_dir = join(second_target_dir, file_name)\n",
    "      f = open(file_dir, \"rb\")\n",
    "      second_hm_list += [pickle.load(f)]\n",
    "      f.close()    \n",
    "\n",
    "\n",
    "    #################### CALCULATE SCORES ####################\n",
    "    class_sensitivity = []\n",
    "    null_player = []\n",
    "\n",
    "    counter = 0\n",
    "    for first_hm, second_hm in zip(first_hm_list, second_hm_list):\n",
    "        index = second_hm_files[counter].split('_')[2]\n",
    "        temp_data = data_list[patches_files.index(index)]\n",
    "\n",
    "        null_player += [get_null_player(first_hm, temp_data).item()]\n",
    "        class_sensitivity += [get_class_sensitivity([first_hm, second_hm], temp_data).item()]\n",
    "\n",
    "        counter += 1\n",
    "\n",
    "    print(m + ': ', np.array(class_sensitivity).mean(), ', ', np.array(null_player).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Method: Null Player')\n",
    "\n",
    "\n",
    "method_dir = '/media/heatmaps/null'\n",
    "methods = ['DeepLiftShap', 'ExtremalPertubation', 'FullGrad', 'GradCAM', 'Gradient',\n",
    "          'GuidedBackProp', 'IntegratedGradients', 'GradCAMPlusPlus/layer1',\n",
    "          'GradCAMPlusPlus/layer2', 'GradCAMPlusPlus/layer3', 'GradCAMPlusPlus/layer4', 'IBA/layer1',\n",
    "          'IBA/layer2', 'IBA/layer3', 'IBA/layer4']\n",
    "\n",
    "\n",
    "for m in methods:\n",
    "    first_target_dir = join(method_dir, m, 'second_target/data')\n",
    "    \n",
    "    #################### READ HEATMAPS ####################\n",
    "    first_hm_files = sorted([f for f in listdir(first_target_dir) if isfile(join(first_target_dir, f))])\n",
    "    first_hm_list = []\n",
    "\n",
    "    for file_name in sorted(first_hm_files):\n",
    "      file_dir = join(first_target_dir, file_name)\n",
    "      f = open(file_dir, \"rb\")\n",
    "      first_hm_list += [pickle.load(f)]\n",
    "      f.close()   \n",
    "\n",
    "    #################### CALCULATE SCORES ####################\n",
    "    null_player = []\n",
    "\n",
    "    counter = 0\n",
    "    for first_hm in zip(first_hm_list):\n",
    "        index = first_hm_files[counter].split('_')[2]\n",
    "        temp_data = data_list[patches_files.index(index)]\n",
    "\n",
    "        null_player += [1/get_null_player(first_hm[0], temp_data).item()]\n",
    "\n",
    "        counter += 1\n",
    "\n",
    "    print(m + ': ', np.array(null_player).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import softmax\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "print(\"Method: single patch\")\n",
    "\n",
    "method_dir = '/media/heatmaps/single/'\n",
    "methods = ['DeepLiftShap', 'ExtremalPertubation', 'FullGrad', 'GradCAM', 'Gradient',\n",
    "          'GuidedBackProp', 'IntegratedGradients', 'GradCAMPlusPlus/layer1',\n",
    "          'GradCAMPlusPlus/layer2', 'GradCAMPlusPlus/layer3', 'GradCAMPlusPlus/layer4', 'IBA/layer1',\n",
    "          'IBA/layer2', 'IBA/layer3', 'IBA/layer4']\n",
    "\n",
    "\n",
    "for m in methods:\n",
    "    first_target_dir = join(method_dir, m, 'target/data')\n",
    "    second_target_dir = join(method_dir, m, 'second_target/data')\n",
    "\n",
    "    #################### READ HEATMAPS ####################\n",
    "    first_hm_files = sorted([f for f in listdir(first_target_dir) if isfile(join(first_target_dir, f))])\n",
    "    first_hm_list = []\n",
    "\n",
    "    for file_name in sorted(first_hm_files):\n",
    "      file_dir = join(first_target_dir, file_name)\n",
    "      f = open(file_dir, \"rb\")\n",
    "      first_hm_list += [pickle.load(f)]\n",
    "      f.close()\n",
    "\n",
    "\n",
    "    second_hm_files = sorted([f for f in listdir(second_target_dir) if isfile(join(second_target_dir, f))])\n",
    "    second_hm_list = []\n",
    "\n",
    "    for file_name in sorted(second_hm_files):\n",
    "      file_dir = join(second_target_dir, file_name)\n",
    "      f = open(file_dir, \"rb\")\n",
    "      second_hm_list += [pickle.load(f)]\n",
    "      f.close()    \n",
    "\n",
    "\n",
    "    #################### CALCULATE SCORES ####################\n",
    "    first_target_scores = []\n",
    "    second_target_scores = []\n",
    "    first_target_random = []\n",
    "    second_target_random = []\n",
    "\n",
    "\n",
    "    # counter = 0\n",
    "    for i in range(len(first_hm_list)):\n",
    "\n",
    "        index = second_hm_files[i].split('_')[2]\n",
    "        temp_data = data_list[patches_files.index(index)]\n",
    "\n",
    "        t1 = single_patch_hm_sum(abs(first_hm_list[i]), temp_data)\n",
    "        t2 = single_patch_hm_sum(abs(second_hm_list[i]), temp_data)\n",
    "        \n",
    "        first_target_scores.append(t1[0])\n",
    "        second_target_scores.append(t2[0])\n",
    "\n",
    "    corr, pval = spearmanr(first_target_scores, second_target_scores)\n",
    "    print(m, ': ', corr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import softmax\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "print(\"Method: Repeated\")\n",
    "\n",
    "method_dir = '/media/heatmaps/repeated-new/'\n",
    "methods = ['DeepLiftShap', 'ExtremalPertubation', 'FullGrad', 'GradCAM', 'Gradient',\n",
    "          'GuidedBackProp', 'IntegratedGradients', 'GradCAMPlusPlus/layer4', 'IBA/layer4']\n",
    "\n",
    "for m in methods:\n",
    "    first_target_dir = join(method_dir, m, 'second_target/data')\n",
    "\n",
    "    #################### READ HEATMAPS ####################\n",
    "    first_hm_files = sorted([f for f in listdir(first_target_dir) if isfile(join(first_target_dir, f))])\n",
    "    first_hm_list = []\n",
    "\n",
    "    for file_name in sorted(first_hm_files):\n",
    "      file_dir = join(first_target_dir, file_name)\n",
    "      f = open(file_dir, \"rb\")\n",
    "      first_hm_list += [pickle.load(f)]\n",
    "      f.close()\n",
    "\n",
    "\n",
    "    #################### CALCULATE SCORES ####################\n",
    "    first_target_scores = []\n",
    "    second_target_scores = []\n",
    "\n",
    "\n",
    "    # counter = 0\n",
    "    for i in range(len(first_hm_list)):\n",
    "\n",
    "        index = first_hm_files[i].split('_')[2]\n",
    "        temp_data = data_list[patches_files.index(index)]\n",
    "        t1, t2 = two_patch_corr_score(abs(first_hm_list[i]), temp_data)\n",
    "               \n",
    "        first_target_scores.append(t1)\n",
    "        second_target_scores.append(t2)\n",
    "\n",
    "\n",
    "    corr, pval = spearmanr(first_target_scores, second_target_scores)\n",
    "    print(m, ': ', corr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import softmax\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "print(\"Method: Repeated\")\n",
    "\n",
    "method_dir = '/media/heatmaps/repeated-new/'\n",
    "methods = ['DeepLiftShap', 'ExtremalPertubation', 'FullGrad', 'GradCAM', 'Gradient',\n",
    "          'GuidedBackProp', 'IntegratedGradients', 'GradCAMPlusPlus/layer4', 'IBA/layer4']\n",
    "\n",
    "for m in methods:\n",
    "    first_target_dir = join(method_dir, m, 'data')\n",
    "\n",
    "\n",
    "    #################### READ HEATMAPS ####################\n",
    "    first_hm_files = sorted([f for f in listdir(first_target_dir) if isfile(join(first_target_dir, f))])\n",
    "    first_hm_list = []\n",
    "\n",
    "    for file_name in sorted(first_hm_files):\n",
    "      file_dir = join(first_target_dir, file_name)\n",
    "      f = open(file_dir, \"rb\")\n",
    "      first_hm_list += [pickle.load(f)]\n",
    "      f.close()\n",
    "\n",
    "    #################### CALCULATE SCORES ####################\n",
    "    first_patch_scores = []\n",
    "    second_patch_scores = []\n",
    "\n",
    "\n",
    "    for i in range(len(first_hm_list)):\n",
    "\n",
    "        index = first_hm_files[i].split('_')[2]\n",
    "        temp_data = data_list[patches_files.index(index)]\n",
    "\n",
    "        p1, p2 = two_patch_corr_score(abs(first_hm_list[i]), temp_data)\n",
    "        first_patch_scores.append(p1)\n",
    "        second_patch_scores.append(p2)\n",
    "\n",
    "    corr, pval = spearmanr(first_patch_scores, second_patch_scores)\n",
    "    print(m, corr)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "l2jiL4BmGcdQ",
    "9HWxkwj4KXmV",
    "r-KWxv-6KrwG",
    "GExOnS88NZoL",
    "qbjozI5gv-yG",
    "4MR1FZRLBHJa",
    "t2SRkLv5VNOv"
   ],
   "name": "Attribution Methods.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "02264d3a2b8e4ba1a71b7e2e357225a8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0b23b604a6ea48f69b0130008fe73572": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0dd547c66aed405a8284d21742c18389": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1468f45d121d4ee1998aabd36ca64ceb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "32e3bce25b82425f84089a6d59fd478d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5be051f7ed1c489fae45d6127e8f8829": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ebc276c765c34f31bbb25fb8133eaa44",
      "placeholder": "​",
      "style": "IPY_MODEL_cee91690cbd44a8a855da67ce1c0a107",
      "value": " 13/13 [00:03&lt;00:00,  3.22it/s]"
     }
    },
    "61f528ba67434ea8bf28d992315e131b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1468f45d121d4ee1998aabd36ca64ceb",
      "max": 46830571,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d65653601f63465db95c275bb66eb80c",
      "value": 46830571
     }
    },
    "822add6865e8404b9137da5827fc6875": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "92c042e83f6f4090b0d8d667b69a5600": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_02264d3a2b8e4ba1a71b7e2e357225a8",
      "placeholder": "​",
      "style": "IPY_MODEL_0b23b604a6ea48f69b0130008fe73572",
      "value": "100%"
     }
    },
    "9ec04d0f8bd344e39e100d1c1fd72c23": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_32e3bce25b82425f84089a6d59fd478d",
      "max": 13,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_822add6865e8404b9137da5827fc6875",
      "value": 13
     }
    },
    "a0ad0c5bb48d4e6c8108ea7365d169da": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0dd547c66aed405a8284d21742c18389",
      "placeholder": "​",
      "style": "IPY_MODEL_aceaeee5bab84f548c7a628726643c10",
      "value": " 44.7M/44.7M [00:03&lt;00:00, 12.4MB/s]"
     }
    },
    "a13fb52ab664465392a464701fbe867e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e2946d57503a4eb680e50b4601d05f7c",
      "placeholder": "​",
      "style": "IPY_MODEL_a813be12cbd14e439ae666417d5ed940",
      "value": "100%"
     }
    },
    "a813be12cbd14e439ae666417d5ed940": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "aceaeee5bab84f548c7a628726643c10": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b61db78ba6f64e57b873498c9e5b7432": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_92c042e83f6f4090b0d8d667b69a5600",
       "IPY_MODEL_9ec04d0f8bd344e39e100d1c1fd72c23",
       "IPY_MODEL_5be051f7ed1c489fae45d6127e8f8829"
      ],
      "layout": "IPY_MODEL_c5fbf08d5ddc405cb224d8f1b481c958"
     }
    },
    "c5fbf08d5ddc405cb224d8f1b481c958": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c6d4bd3ab87249b39e848b2376defa5e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cee91690cbd44a8a855da67ce1c0a107": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d65653601f63465db95c275bb66eb80c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e2946d57503a4eb680e50b4601d05f7c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ebc276c765c34f31bbb25fb8133eaa44": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ed66bd634cdf419b809ba2b36d18b27d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a13fb52ab664465392a464701fbe867e",
       "IPY_MODEL_61f528ba67434ea8bf28d992315e131b",
       "IPY_MODEL_a0ad0c5bb48d4e6c8108ea7365d169da"
      ],
      "layout": "IPY_MODEL_c6d4bd3ab87249b39e848b2376defa5e"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
